{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "current_path = Path.cwd()\n",
    "root = current_path.parent\n",
    "sys.path.append(str(root))\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Normalization, RandomFlip, RandomRotation, RandomZoom, Dense, Dropout, Embedding\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Reshape, Rescaling\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        Normalization(),\n",
    "        Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "        RandomFlip(\"horizontal\"),\n",
    "        RandomRotation(factor=0.02),\n",
    "        RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "        Rescaling(1./255, input_shape=(28, 28, 1))\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layers.selective_attention import SelectiveAttention\n",
    "\n",
    "def positional_encoding_2D(length, width, depth):\n",
    "    depth = depth / 4\n",
    "\n",
    "    pos_x = np.arange(width)[:, np.newaxis]\n",
    "    pos_y = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
    "\n",
    "    angle_rates = 1 / (10000 ** depths)\n",
    "    angle_rads_x = pos_x * angle_rates\n",
    "    angle_rads_y = pos_y * angle_rates\n",
    "\n",
    "    pos_encoding_x = np.concatenate(\n",
    "        [np.sin(angle_rads_x), np.cos(angle_rads_x)],\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    pos_encoding_x = tf.expand_dims(pos_encoding_x, 1)\n",
    "\n",
    "    pos_encoding_y = np.concatenate(\n",
    "        [np.sin(angle_rads_y), np.cos(angle_rads_y)],\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    pos_encoding_y = tf.expand_dims(pos_encoding_y, 0)\n",
    "\n",
    "    pos_encoding_x = tf.tile(pos_encoding_x, (1, length, 1))\n",
    "    pos_encoding_y = tf.tile(pos_encoding_y, (width, 1, 1))\n",
    "    pos_encoding = tf.concat([pos_encoding_x, pos_encoding_y], -1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, length, width, d_model):\n",
    "        super().__init__()\n",
    "        self._length = length\n",
    "        self._width = width\n",
    "        self._input_dim = length * width\n",
    "        self._d_model = d_model\n",
    "        self._embedding = tf.keras.layers.Embedding(input_dim=self._input_dim, output_dim=self._d_model, mask_zero=True)\n",
    "        self._pos_encoding = positional_encoding_2D(length=self._length, width=self._width, depth=self._d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self._embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self._d_model, tf.float32))\n",
    "        x = x + self._pos_encoding[tf.newaxis, :self._length, :self._width, :]\n",
    "        return x\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self._sa = SelectiveAttention(**kwargs)\n",
    "        self._layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self._add = tf.keras.layers.Add()\n",
    "\n",
    "#automatic and triggered by stimulis\n",
    "class BottomUpAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        att_output, att_scores = self._sa(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            return_attention_scores=True\n",
    "        )\n",
    "\n",
    "        x = self._add([x, att_output])\n",
    "        x = self._layer_norm(x)\n",
    "        self._last_att_scores = att_scores\n",
    "        return x\n",
    "    \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self._seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_model, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "            ])\n",
    "        self._add = tf.keras.layers.Add()\n",
    "        self._layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self._add([x, self._seq(x)])\n",
    "        x = self._layer_norm(x)\n",
    "        return x\n",
    "    \n",
    "class FinalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_units, d_model, hidden_rate, num_cat, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self._flatten = tf.keras.layers.Flatten()\n",
    "        self._seq = tf.keras.Sequential([\n",
    "            #tf.keras.layers.Dropout(hidden_rate),\n",
    "            #tf.keras.layers.Dense(hidden_units, activation=\"gelu\"),\n",
    "            tf.keras.layers.Dense(d_model, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "            ])\n",
    "        self._add = tf.keras.layers.Add()\n",
    "        self._layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self._final = tf.keras.layers.Dense(num_cat)\n",
    "\n",
    "    def call(self, x):\n",
    "        reduce_axis = range(1, x.shape.rank - 1)\n",
    "        x = tf.reduce_mean(x, axis=reduce_axis, keepdims=False)\n",
    "        x = self._add([x, self._seq(x)])\n",
    "        x = self._layer_norm(x)\n",
    "        x = self._final(x)\n",
    "        return x\n",
    "    \n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self._bottom_up_attention = BottomUpAttention(\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self._ffn = FeedForward(d_model=d_model, dropout_rate=dropout_rate)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self._bottom_up_attention(x)\n",
    "        x = self._ffn(x)\n",
    "        self._last_att_scores = self._bottom_up_attention._last_att_scores\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 num_layers,\n",
    "                 d_model,\n",
    "                 length,\n",
    "                 width,\n",
    "                 hidden_units,\n",
    "                 num_cat,\n",
    "                 hidden_rate=0.5,\n",
    "                 dropout_rate=0.1 \n",
    "                ):\n",
    "        super().__init__()\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_units = hidden_units\n",
    "        self._num_cat = num_cat\n",
    "        self._pos_embedding  = PositionalEmbedding2D(length, width, d_model)\n",
    "        self._attention_layer = [\n",
    "            AttentionLayer(\n",
    "                d_model=d_model,\n",
    "                dropout_rate=dropout_rate\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self._dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self._final_layer = FinalLayer(hidden_units=hidden_units, d_model=d_model, \n",
    "                                       hidden_rate=hidden_rate, num_cat=num_cat, dropout_rate=dropout_rate)\n",
    "        self._last_att_score = None\n",
    "    \n",
    "    def call(self, x):\n",
    "        #x = tf.expand_dims(x, -1)\n",
    "        x = self._pos_embedding(x)\n",
    "        x = self._dropout(x)\n",
    "\n",
    "        for i in range(self._num_layers):\n",
    "            x = self._attention_layer[i](x)\n",
    "\n",
    "        self._last_att_score = self._attention_layer[-1]._last_att_scores\n",
    "\n",
    "        logits = self._final_layer(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 12\n",
    "d_model = 16\n",
    "length = 28\n",
    "width = 28\n",
    "hidden_units=1024\n",
    "num_cat = 10\n",
    "hidden_rate = 0.5\n",
    "dropout_rate = 0.1\n",
    "\n",
    "model = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    length=length,\n",
    "    width=width,\n",
    "    hidden_units=hidden_units,\n",
    "    num_cat=num_cat,\n",
    "    hidden_rate=hidden_rate,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "history = model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics = ['accuracy'],\n",
    "    run_eagerly = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience=patience\n",
    "        self.best_accuracy = 0\n",
    "        self.stagnation_counter = 0\n",
    "\n",
    "    def on_epooch_end(self, epoch, logs=None):\n",
    "        current_accuracy = logs.get(\"accuracy\")\n",
    "\n",
    "        if current_accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = current_accuracy\n",
    "            self.stagnation_counter = 0\n",
    "        else:\n",
    "            self.stagnation_counter += 1\n",
    "\n",
    "        if self.stagnation_counter >= self.patience:\n",
    "            current_lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, current_lr * 1.5)\n",
    "            self.stagnation_counter = 0\n",
    "\n",
    "checkpoint_filepath=\"./tmp/vsa12.weights.h5\"\n",
    "\n",
    "callback = [\n",
    "\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_accuracy',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    ),\n",
    "\n",
    "    CustomCallback(patience=5)\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 256\n",
    "num_epochs = 50\n",
    "\n",
    "history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks= callback\n",
    "    )\n",
    "\n",
    "model.load_weights(checkpoint_filepath)\n",
    "_, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy:{round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\"+item], label=\"val_\"+item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history, \"loss\")\n",
    "plot_history(history, \"accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
