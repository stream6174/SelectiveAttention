# SelectiveAttention

the <mark style="background-color:"ccccff"><B>selective attention</B></mark> is implemented based on <B>Feature-Integration Theory</B> proposed by Treisman in 1980.

As a replacement of <mark style="background-color:"ffd400">multi-head attention</mark> described in the paper - <I>Attention is all you need</I>, the selective attention considered the early V.S. late selection debate in cognitive neuroscience, and rebuild the attention following the selection procesure hypothesis. The code devloped following the basic usage of Keras multi-head attention layers, it is ~10x faster and same accuracy in experiment using MNIST Fashion dataset, and tested to save more than 90% memory on ImageNet-1k classifcication task to reach the same accuracy.

Looking forward for your testing results on other experiments.

