# SelectiveAttention
the selective attention block based on information integration theory.

As a replacement block of multi-head attention described in the paper - Attention is all you need, the selective attention block considered the early V.S. late selection debate in cognitive neuroscience, and rebuild the attention block based on information integration theory proposed by Treisman in 1980. The code devloped following the basic usage of Keras multi-head attention layers, while it is ~10x faster and same accuracy in experiment using MNIST Fashion dataset. Looking forward for your testing results of other experiments.

